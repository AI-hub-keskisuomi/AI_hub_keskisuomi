{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NilSkBKhPthJ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "device = torch.device('cuda', 1)\n",
    "# device = torch.device()\n",
    "print(device)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "import torch.utils.data as data\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "\n",
    "from sklearn import decomposition\n",
    "from sklearn import manifold\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import copy\n",
    "from collections import namedtuple\n",
    "import random\n",
    "import shutil\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4QmwmcXuPuLo"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Image data should be organized like this:\n",
    "\n",
    "KSSHP-dataset\n",
    "|----ALL image tiles\n",
    "|     |--train_val_data (90 % of ALL image tiles)\n",
    "|     |      |----TUM\n",
    "|     |      |----STR\n",
    "|     |      |----OTHER\n",
    "|     |      \n",
    "|     |--train_data (2/3 of train_val_data)\n",
    "|     |      |----TUM\n",
    "|     |      |----STR\n",
    "|     |      |----OTHER\n",
    "|     |--val_data (1/3 of train_val_data)\n",
    "|     |      |----TUM\n",
    "|     |      |----STR\n",
    "|     |      |----OTHER\n",
    "|     |\n",
    "|     |--test_data (10 % of ALL image tiles)\n",
    "|     |      |----TUM\n",
    "|     |      |----STR\n",
    "|     |      |----OTHER\n",
    "\n",
    "Kather-dataset\n",
    "|----train_val_data_kather\n",
    "|     |--TUM\n",
    "|     |--STR\n",
    "|     |--OTHER\n",
    "|     \n",
    "|----train_data_kather (2/3 of train_val_data_kather)\n",
    "|     |--TUM\n",
    "|     |--STR\n",
    "|     |--OTHER\n",
    "|\n",
    "|----val_data_kather (1/3 of train_val_data_kather)\n",
    "|     |--TUM\n",
    "|     |--STR\n",
    "|     |--OTHER\n",
    "\n",
    "\"\"\"\n",
    "# ksshp_root: root to ALL training data from private dataset (in this case KSSHP)\n",
    "# kather_root: root to image tiles from Kather-datasest\n",
    "# SETUP: which setup to use (1, 2 or 3):\n",
    "#      - SETUP 1: pre-training with ImageNet and Kather-data before KSSHP-data\n",
    "#      - SETUP 2: pre-training only with ImageNet before KSSHP-data\n",
    "#      - SETUP 3: pre-training only with Kather-data without ImageNet\n",
    "\n",
    "\n",
    "\n",
    "# normalize image tiles and transform them into pytorch-tensor\n",
    "# returns data iterators for train_val, train, val and test images\n",
    "\n",
    "def make_train_val_test_groups(ksshp_root, kather_root, SETUP):\n",
    "\n",
    "    torch.manual_seed(1234)\n",
    "    torch.cuda.manual_seed(1234)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    batch = 32\n",
    "    pretrained_size = 224\n",
    "    means = [0.485, 0.456, 0.406]\n",
    "    stds= [0.229, 0.224, 0.225]\n",
    "\n",
    "\n",
    "    data_transforms = transforms.Compose([transforms.Resize(pretrained_size),\n",
    "                                          transforms.ToTensor(),\n",
    "                                          transforms.Normalize(mean = means, std = stds)])\n",
    "\n",
    "    train_val_data = datasets.ImageFolder(root = os.path.join(ksshp_root, \"tiles_224_balanced/\"),\n",
    "                                  transform = data_transforms)\n",
    "    \n",
    "    train_data = datasets.ImageFolder(root = os.path.join(ksshp_root, \"tiles_224_TRAIN/\"),\n",
    "                                  transform = data_transforms)\n",
    "    \n",
    "    val_data = datasets.ImageFolder(root = os.path.join(ksshp_root, \"tiles_224_VAL/\"),\n",
    "                                  transform = data_transforms)\n",
    "    \n",
    "    test_data = datasets.ImageFolder(root = os.path.join(ksshp_root, \"tiles_224_test/\"),\n",
    "                                  transform = data_transforms)\n",
    "    \n",
    "    if (SETUP == 3):\n",
    "        \n",
    "        means = torch.zeros(3)\n",
    "        stds = torch.zeros(3)\n",
    "\n",
    "        for img, label in train_data:\n",
    "            means += torch.mean(img, dim = (1,2))\n",
    "            stds += torch.std(img, dim = (1,2))\n",
    "\n",
    "        means /= len(train_data)\n",
    "        stds /= len(train_data)\n",
    "        print(f'Means: {means}')\n",
    "        print(f'Stds: {stds}')\n",
    "\n",
    "        train_val_data = datasets.ImageFolder(root = os.path.join(ksshp_root, \"ALL/train_val_data/\"),\n",
    "                                      transform = data_transforms)\n",
    "\n",
    "        train_data = datasets.ImageFolder(root = os.path.join(ksshp_root, \"ALL/train_data/\"),\n",
    "                                      transform = data_transforms)\n",
    "\n",
    "        val_data = datasets.ImageFolder(root = os.path.join(ksshp_root, \"ALL/val_data/\"),\n",
    "                                      transform = data_transforms)\n",
    "\n",
    "        test_data = datasets.ImageFolder(root = os.path.join(ksshp_root, \"ALL/test_data/\"),\n",
    "                                      transform = data_transforms)\n",
    "    \n",
    "\n",
    "    print(f'Training and validation images: {len(train_val_data)}')\n",
    "    print(f'Training images: {len(train_data)}')\n",
    "    print(f'Validation images: {len(val_data)}')\n",
    "    print(f'Test images: {len(test_data)}')\n",
    "    \n",
    "    return train_val_data, train_data, val_data, test_data\n",
    "\n",
    "def make_iterator(dataset, batch):\n",
    "    \n",
    "    iterator = data.DataLoader(dataset, shuffle = True, batch_size = batch)\n",
    "\n",
    "    return iterator\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_data, train_data, val_data, test_data = make_train_val_test_groups(\"/n/archive00/labs/IT/JYU_AIHUB/HE20x/\", \"_\", 1)\n",
    "train_val_iterator = make_iterator(train_val_data, 32)\n",
    "train_iterator = make_iterator(train_data, 32)\n",
    "val_iterator = make_iterator(val_data, 32)\n",
    "test_iterator = make_iterator(test_data, 32)\n",
    "    \n",
    "plot_image_examples(val_data)\n",
    "print(val_dataset[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XZVxkhKxBkYD"
   },
   "outputs": [],
   "source": [
    "def plot_image_examples(dataset):\n",
    "    \n",
    "    idxs = np.arange(0,len(dataset),1, dtype='int')\n",
    "    idxs = list(idxs)\n",
    "    sampleidxs = random.choices(idxs, k=20)\n",
    "    classes = dataset.classes\n",
    "    images, labels = zip(*[(img, label) for img, label in [dataset[sampleidxs[i]] for i in range(len(sampleidxs))]])\n",
    "\n",
    "    rows = int(np.sqrt(len(sampleidxs)))\n",
    "    cols = int(np.sqrt(len(sampleidxs)))\n",
    "\n",
    "    fig = plt.figure(figsize = (15, 15))\n",
    "\n",
    "    for j in range(rows*cols):\n",
    "\n",
    "        ax = fig.add_subplot(rows, cols, j+1)\n",
    "        image = images[j]\n",
    "        image_min = image.min()\n",
    "        image_max = image.max()\n",
    "        image.clamp_(min = image_min, max = image_max)\n",
    "        image.add_(-image_min).div_(image_max - image_min + 1e-5)\n",
    "        ax.imshow(image.permute(1, 2, 0).cpu().numpy())\n",
    "        label = classes[labels[j]]\n",
    "        ax.set_title(label)\n",
    "        ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t_EKb_cNadbI"
   },
   "outputs": [],
   "source": [
    "def calc_acc(y_pred, y):\n",
    "    with torch.no_grad():\n",
    "        batch_size = y.shape[0]\n",
    "        _, top_pred = y_pred.topk(1, 1)\n",
    "        top_pred = top_pred.t()\n",
    "        correct = top_pred.eq(y.view(1, -1).expand_as(top_pred))\n",
    "        correct = correct[:1].reshape(-1).float().sum(0, keepdim = True)\n",
    "        acc = correct / batch_size\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qr6Je0jMafsp"
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, scheduler, device):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train()\n",
    "    \n",
    "    for (x, y) in iterator:\n",
    "        \n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x)\n",
    "        loss = criterion(y_pred, y)\n",
    "        acc = calc_acc(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    epoch_loss /= len(iterator)\n",
    "    epoch_acc /= len(iterator)\n",
    "        \n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "85FW74KaakI_"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion, device):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for (x, y) in iterator:\n",
    "\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            y_pred = model(x)\n",
    "            loss = criterion(y_pred, y)\n",
    "            acc = calc_acc(y_pred, y)\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    epoch_loss /= len(iterator)\n",
    "    epoch_acc /= len(iterator)\n",
    "        \n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wkR6LzakamOV"
   },
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "colab_type": "code",
    "id": "lvoyX823apEN",
    "outputId": "0cb310cb-9016-4cca-dcbb-df2f04345e99"
   },
   "outputs": [],
   "source": [
    "# this is an example using pretrained alexnet\n",
    "\n",
    "def prepare_model(number_of_classes):\n",
    "    \n",
    "    pretrained_model = models.alexnet(pretrained = True)  \n",
    "    in_feat = pretrained_model.classifier[6].in_features \n",
    "    out_dim = number_of_classes\n",
    "    fc = nn.Linear(in_feat, out_dim)\n",
    "\n",
    "    pretrained_model.fc = fc\n",
    "    model = pretrained_model\n",
    "\n",
    "    return model\n",
    "\n",
    "def cross_validate(epochs, train_data, train_iterator):\n",
    "    \n",
    "    best_loss = float('inf')\n",
    "    best_acc = float('inf')\n",
    "    k_folds = 5\n",
    "    steps = len(train_iterator)\n",
    "    total_steps = epochs * steps * k_folds\n",
    "    training_times = np.zeros((epochs, 2))\n",
    "    \n",
    "    kfold = KFold(n_splits=k_folds, shuffle=True)\n",
    "\n",
    "    for fold, (train_ids, val_ids) in enumerate(kfold.split(train_data)):\n",
    "\n",
    "        model = prepare_model(train_data.classes)\n",
    "        \n",
    "        LR = [1e-3, 2e-3, 3e-3, 4e-3, 5e-3]\n",
    "\n",
    "        # alexnet\n",
    "        params = [\n",
    "                  {'params': model.features[0].parameters(), 'lr': LR[fold] / 10},\n",
    "                  {'params': model.features[3].parameters(), 'lr': LR[fold] / 8},\n",
    "                  {'params': model.features[6].parameters(), 'lr': LR[fold] / 6},\n",
    "                  {'params': model.features[8].parameters(), 'lr': LR[fold] / 4},\n",
    "                  {'params': model.features[10].parameters(), 'lr': LR[fold] / 2},\n",
    "                  {'params': model.fc.parameters()}\n",
    "                 ]\n",
    "\n",
    "        model.load_state_dict(model_.state_dict())\n",
    "        # if you already have a pre-trained model using some other dataset, you can load it instead:\n",
    "    #     model.load_state_dict(torch.load('SETUP_1_alexnet_PUBLIC_done.pt'))\n",
    "\n",
    "        if(fold==0):    \n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            optimizer = optim.Adam(params, lr = LR[fold])\n",
    "            op = \"Adam\"\n",
    "\n",
    "        if(fold==1):\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            optimizer = optim.SGD(params, lr = LR[fold])\n",
    "            op = \"SGD\"\n",
    "\n",
    "        if(fold==2):\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            optimizer = optim.SGD(params, lr = LR[fold])\n",
    "            op = \"SGD\"\n",
    "\n",
    "        if(fold==3):\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            optimizer = optim.Adam(params, lr = LR[fold])\n",
    "            op = \"Adam\"\n",
    "\n",
    "        if(fold==4):\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            optimizer = optim.SGD(params, lr = LR[fold])\n",
    "            op = \"SGD\"\n",
    "\n",
    "        model = model.to(device)\n",
    "        criterion = criterion.to(device)\n",
    "\n",
    "        max_lrs = [p['lr'] for p in optimizer.param_groups]\n",
    "        scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr = max_lrs, total_steps = total_steps)\n",
    "\n",
    "\n",
    "        print(\"Alexnet\")\n",
    "        print(f'FOLD {fold}')\n",
    "        print('--------------------------------')\n",
    "\n",
    "        train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "        val_subsampler = torch.utils.data.SubsetRandomSampler(val_ids)\n",
    "\n",
    "        trainloader = torch.utils.data.DataLoader(train_data, batch_size=32, sampler=train_subsampler)\n",
    "        val_loader = torch.utils.data.DataLoader(train_data,batch_size=32, sampler=val_subsampler)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "                start_time = time.monotonic()\n",
    "\n",
    "                train_loss, train_acc = train(model, trainloader, optimizer, criterion, scheduler, device)\n",
    "                val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
    "\n",
    "                end_time = time.monotonic()\n",
    "\n",
    "                epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "                training_times[epoch-1,0] = epoch_mins\n",
    "                training_times[epoch-1,1] = epoch_secs\n",
    "\n",
    "                print(\"FOLD \"+ str(fold)+\": \")\n",
    "                print(\"-------------------------\")\n",
    "                print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "                print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:6.2f}% | ')\n",
    "                print(f'\\tValidation Loss: {val_loss:.3f} | Validation Acc: {train_acc*100:6.2f}% | ')\n",
    "\n",
    "                if val_loss < best_loss:\n",
    "                        best_loss = val_loss\n",
    "                        best_acc = val_acc\n",
    "                        from_fold = fold\n",
    "                        with_op = op\n",
    "                        learning_rate = LR[fold]\n",
    "\n",
    "        print(f'\\n Fold {fold}: Best validation loss: {best_loss:.3f} | Best validation acc: {best_acc*100:6.2f}% | ')\n",
    "\n",
    "        averages = np.mean(training_times, axis = 0)\n",
    "        average_mins = averages[0]\n",
    "        average_secs = averages[1]\n",
    "\n",
    "    print(\"Best params in fold\"+str(from_fold)+\" with learning rate \"+str(learning_rate)+\", optimizer \"+with_op)\n",
    "    \n",
    "    return op, learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# to choose the best possible amount of epochs for the final model, train the model with\n",
    "# chosen parameters (above) at least for 40 epochs or so and plot the results. \n",
    "# Use train- and validation -datasets.\n",
    "\n",
    "def train_and_plot(train_iterator, val_iterator, optimizer, epochs, found_lr):\n",
    "    \n",
    "    best_loss = float('inf')\n",
    "    best_acc = float('inf')\n",
    "\n",
    "    losses_train = []\n",
    "    losses_val = []\n",
    "    acc_train = []\n",
    "    acc_val = [] \n",
    "\n",
    "    steps = len(train_iterator)\n",
    "    total_steps = epochs * steps * k_folds\n",
    "\n",
    "    training_times = np.zeros((epochs, 2))\n",
    "\n",
    "    model = prepare_model(train_data.classes)\n",
    "\n",
    "    params = [\n",
    "                  {'params': model.features[0].parameters(), 'lr': found_lr / 10},\n",
    "                  {'params': model.features[3].parameters(), 'lr': found_lr / 8},\n",
    "                  {'params': model.features[6].parameters(), 'lr': found_lr / 6},\n",
    "                  {'params': model.features[8].parameters(), 'lr': found_lr / 4},\n",
    "                  {'params': model.features[10].parameters(), 'lr': found_lr / 2},\n",
    "                  {'params': model.fc.parameters()}\n",
    "                 ]\n",
    "\n",
    "    if (optimizer == \"Adam\"):\n",
    "        optimizer = optim.Adam(params, lr = found_lr)\n",
    "        \n",
    "    if (optimizer == \"SGD\"):\n",
    "        optimizer = optim.SGD(params, lr = found_lr)\n",
    "        \n",
    "    model.load_state_dict(model_.state_dict())\n",
    "    # model.load_state_dict(torch.load('SETUP_1_alexnet_PUBLIC_done.pt'))\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model = model.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "\n",
    "    max_lrs = [p['lr'] for p in optimizer.param_groups]\n",
    "    scheduler = lr_scheduler.OneCycleLR(optimizer,max_lr = max_lrs, total_steps = total_steps)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        start_time = time.monotonic()\n",
    "\n",
    "        train_loss, train_acc = train(model, train_iterator, optimizer, criterion, scheduler, device)\n",
    "        val_loss, val_acc = evaluate(model, val_iterator, criterion, device)\n",
    "\n",
    "        end_time = time.monotonic()\n",
    "\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "        training_times[epoch-1,0] = epoch_mins\n",
    "        training_times[epoch-1,1] = epoch_secs\n",
    "\n",
    "        print(\"Alexnet\")\n",
    "        print(\"-------------------------\")\n",
    "        print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'\\tTrain loss: {train_loss:.3f} | Train accuracy: {train_acc*100:6.2f}% | ')\n",
    "        print(f'\\n Validation loss: {val_loss:.3f} | Validation accuracy: {val_acc*100:6.2f}% | ')\n",
    "\n",
    "        losses_train.append(train_loss)\n",
    "        losses_val.append(val_loss)\n",
    "        acc_train.append(train_acc)\n",
    "        acc_val.append(val_acc)\n",
    "\n",
    "        epochs1 = range(1, len(losses_train)+1)\n",
    "        epochs2 = range(1, len(acc_train)+1)\n",
    "\n",
    "        plt.plot(epochs1, losses_train, label='Training loss')\n",
    "        plt.plot(epochs1, losses_val, label='Validation loss')\n",
    "        plt.xticks(np.arange(0, len(epochs2)+1, 1))\n",
    "        plt.yticks(np.arange(0, 1, 0.1))\n",
    "\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "        plt.plot(epochs2, acc_train, label='Training accuracy')\n",
    "        plt.plot(epochs2, acc_val, label='Validation accuracy')\n",
    "        plt.xticks(np.arange(0, len(epochs2)+1, 1))\n",
    "        plt.yticks(np.arange(0, 1, 0.1))\n",
    "\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    averages = np.mean(training_times, axis = 0)\n",
    "    average_mins = averages[0]\n",
    "    average_secs = averages[1]\n",
    "    \n",
    "    results = [losses_train, losses_val, acc_train, acc_val]\n",
    "    \n",
    "    for i in range(len(results)):\n",
    "        np.save(\"./\", results[i])\n",
    "        \n",
    "    print(\"Average training time/epoch: \" + str(average_mins) + \" m, \" + str(average_secs) + \" s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer, learning_rate = cross_validate(10, test_data, test_iterator)\n",
    "train_and_plot(train_iterator, val_iterator, optimizer, 40, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train_final(train_val_iterator, epochs, optimizer, found_lr, save_root):\n",
    "    \n",
    "    steps = len(train_val_iterator)\n",
    "    total_steps = epochs * steps\n",
    "\n",
    "    training_times = np.zeros((epochs, 2))\n",
    "\n",
    "    model = prepare_model(3)\n",
    "\n",
    "    params = [\n",
    "                  {'params': model.features[0].parameters(), 'lr': found_lr / 10},\n",
    "                  {'params': model.features[3].parameters(), 'lr': found_lr / 8},\n",
    "                  {'params': model.features[6].parameters(), 'lr': found_lr / 6},\n",
    "                  {'params': model.features[8].parameters(), 'lr': found_lr / 4},\n",
    "                  {'params': model.features[10].parameters(), 'lr': found_lr / 2},\n",
    "                  {'params': model.fc.parameters()}\n",
    "                 ]\n",
    "\n",
    "    if (optimizer == \"Adam\"):\n",
    "        optimizer = optim.Adam(params, lr = found_lr)\n",
    "        \n",
    "    if (optimizer == \"SGD\"):\n",
    "        optimizer = optim.SGD(params, lr = found_lr)\n",
    "\n",
    "    model.load_state_dict(model.state_dict())\n",
    "    \n",
    "    # model.load_state_dict(torch.load('SETUP_1_alexnet_PUBLIC_done.pt'))\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model = model.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "\n",
    "    max_lrs = [p['lr'] for p in optimizer.param_groups]\n",
    "    scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr = max_lrs, total_steps = total_steps)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        start_time = time.monotonic()\n",
    "\n",
    "        train_loss, train_acc = train(model, train_iterator, optimizer, criterion, scheduler, device)\n",
    "\n",
    "        end_time = time.monotonic()\n",
    "\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "        training_times[epoch-1,0] = epoch_mins\n",
    "        training_times[epoch-1,1] = epoch_secs\n",
    "\n",
    "        print(\"Alexnet FINAL model\")\n",
    "        print(\"-------------------------\")\n",
    "        print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc @1: {train_acc_1*100:6.2f}% | ')\n",
    "        \n",
    "    averages = np.mean(training_times, axis = 0)\n",
    "    average_mins = averages[0]\n",
    "    average_secs = averages[1]\n",
    "\n",
    "    torch.save(model.state_dict(), os.path.join(save_root, \"alexnet_model.pt\"))\n",
    "    print(f\"Model training complete, saved to: {save_root}\")\n",
    "    print(f\"Average training time/epoch: {average_mins} m, {average_secs} s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the amount of epochs based on the plots above, then train the final model\n",
    "# with all training data (train_val_data)\n",
    "train_final(train_val_iterator, 5, optimizer, learning_rate, \"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AlCBiYxQbi4s"
   },
   "outputs": [],
   "source": [
    "def get_predictions(model, iterator):\n",
    "\n",
    "    model.eval()\n",
    "    images = []\n",
    "    labels = []\n",
    "    probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for (x, y) in iterator:\n",
    "\n",
    "            x = x.to(device)\n",
    "            y_pred = model(x)\n",
    "            y_prob = F.softmax(y_pred, dim = -1)\n",
    "            top_pred = y_prob.argmax(1, keepdim = True)\n",
    "            images.append(x.cpu())\n",
    "            labels.append(y.cpu())\n",
    "            probs.append(y_prob.cpu())\n",
    "\n",
    "    images = torch.cat(images, dim = 0)\n",
    "    labels = torch.cat(labels, dim = 0)\n",
    "    probs = torch.cat(probs, dim = 0)\n",
    "\n",
    "    return images, labels, probs\n",
    "\n",
    "def plot_confusion_matrix(labels, pred_labels, classes):\n",
    "    \n",
    "    fig = plt.figure(figsize = (5, 5));\n",
    "    ax = fig.add_subplot(1, 1, 1);\n",
    "    cm = confusion_matrix(labels, pred_labels);\n",
    "    cm = ConfusionMatrixDisplay(cm, display_labels = classes);\n",
    "    cm.plot(cmap = 'Blues', ax = ax)\n",
    "    fig.delaxes(fig.axes[1]) #delete colorbar\n",
    "    plt.xticks(rotation = 90)\n",
    "    plt.xlabel('Predicted Label', fontsize = 10)\n",
    "    plt.ylabel('True Label', fontsize = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "fOla2axMBkZk",
    "outputId": "a8178c45-adaa-4b00-bf59-0a1de0eb4363"
   },
   "outputs": [],
   "source": [
    "def test_model_plot_results(root_to_model, test_iterator, classes):\n",
    "    \n",
    "    model = prepare_model(len(classes))\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    model.load_state_dict(torch.load(root_to_model))\n",
    "    model = model.to(device)\n",
    "    criterion.to(device)\n",
    "    \n",
    "    test_loss, test_acc = evaluate(model, test_iterator, criterion, device)\n",
    "    print(f'Test loss: {test_loss:.3f} | Test accuracy: {test_acc*100:6.2f}% | ')\n",
    "    \n",
    "    images, labels, probs = get_predictions(model, test_iterator)\n",
    "    pred_labels = torch.argmax(probs, 1)\n",
    "    plot_confusion_matrix(labels, pred_labels, classes)\n",
    "\n",
    "test_model_plot_results(\"./SETUP_1_alexnet_FINAL.pt\", test_iterator, test_data.classes)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "5 - ResNet.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "2bfc54026555415b819aef84868882a0": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "33af98c97e6141f3bfc08f9cedb4dbd0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6237aa6a8e1843789e503390f5dcbd0d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "73733f1d2af0455890e9d6f695321d89": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9e26aa8e760c41c4bcd81d121abb8a54": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2bfc54026555415b819aef84868882a0",
      "max": 102502400,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_dcd14c4f39ba40b1924df3921551a759",
      "value": 102502400
     }
    },
    "d1868678a8d2487a85394a7eb6d85608": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6237aa6a8e1843789e503390f5dcbd0d",
      "placeholder": "​",
      "style": "IPY_MODEL_33af98c97e6141f3bfc08f9cedb4dbd0",
      "value": " 97.8M/97.8M [00:00&lt;00:00, 114MB/s]"
     }
    },
    "dcd14c4f39ba40b1924df3921551a759": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "e0d5f6ba52604352a6484d1907385629": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9e26aa8e760c41c4bcd81d121abb8a54",
       "IPY_MODEL_d1868678a8d2487a85394a7eb6d85608"
      ],
      "layout": "IPY_MODEL_73733f1d2af0455890e9d6f695321d89"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
